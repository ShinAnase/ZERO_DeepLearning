{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import Vol6Utl as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key]*grads[key]\n",
    "            params[key] -= self.lr * grads[key]/(np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight_init_activation_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000個のデータ\n",
    "node_num = 100  # 各隠れ層のノード（ニューロン）の数\n",
    "hidden_layer_size = 5  # 隠れ層が5層\n",
    "activations = {}  # ここにアクティベーションの結果を格納する\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 初期値の値をいろいろ変えて実験しよう！\n",
    "    #w = np.random.randn(node_num, node_num) * 1\n",
    "    #w = np.random.randn(node_num, node_num) * 0.01\n",
    "    #w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 活性化関数の種類も変えて実験しよう！\n",
    "    #z = sigmoid(a)\n",
    "    z = ReLU(a)\n",
    "    #z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# ヒストグラムを描画\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTによる比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c64b0d482246>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mDLfromscratch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mDLfromscratch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_AI\\Scripts\\python\\ZERO_DeepLearning\\DLfromscratch\\common\\layers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# coding: utf-8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mim2col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol2im\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DLfromscratch.dataset.mnist import load_mnist\n",
    "\n",
    "def smooth_curve(x):\n",
    "    \"\"\"損失関数のグラフを滑らかにするために用いる\n",
    "\n",
    "    参考：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
    "    \"\"\"\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from DLfromscratch.common.layers import *\n",
    "from DLfromscratch.common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class MultiLayerNet:\n",
    "    \"\"\"全結合による多層ニューラルネットワーク\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    weight_decay_lambda : Weight Decay（L2ノルム）の強さ\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.params = {}\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # レイヤの生成\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "            self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \"\"\"重みの初期値設定\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "            'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "            'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
    "\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        損失関数の値\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
